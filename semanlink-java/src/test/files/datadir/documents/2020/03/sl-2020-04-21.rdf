<!DOCTYPE rdf:RDF [
  <!ENTITY skos 'http://www.w3.org/2004/02/skos/core#'>
  <!ENTITY sl 'http://www.semanlink.net/2001/00/semanlink-schema#'>
  <!ENTITY tag 'http://www.semanlink.net/tag/'>
  <!ENTITY rdf 'http://www.w3.org/1999/02/22-rdf-syntax-ns#'>
  <!ENTITY dc 'http://purl.org/dc/elements/1.1/'>]>
<rdf:RDF
    xmlns:rdf="&rdf;"
    xmlns:dc="&dc;"
    xmlns:skos="&skos;"
    xmlns:sl="&sl;"
    xmlns:tag="&tag;">
  <rdf:Description rdf:about="bert_elmo_gpt_2_how_contex">
    <sl:tag rdf:resource="&tag;emnlp_2019"/>
    <sl:creationDate>2020-03-28</sl:creationDate>
    <sl:creationTime>2020-03-28T10:33:17Z</sl:creationTime>
    <sl:bookmarkOf rdf:resource="http://ai.stanford.edu/blog/contextual/"/>
    <dc:title>BERT, ELMo, &amp; GPT-2: How Contextual are Contextualized Word Representations? | SAIL Blog</dc:title>
    <sl:tag rdf:resource="&tag;contextualised_word_representations"/>
    <sl:tag rdf:resource="&tag;nlp_stanford"/>
    <sl:tag rdf:resource="&tag;bertology"/>
  </rdf:Description>
  <rdf:Description rdf:about="adrian_gschwend_sur_twitter_">
    <sl:creationTime>2020-03-12T12:38:34Z</sl:creationTime>
    <sl:creationDate>2020-03-12</sl:creationDate>
    <dc:title>Adrian Gschwend sur Twitter : "getting started with RDF and JavaScript!..."</dc:title>
    <sl:tag rdf:resource="&tag;javascript_rdf"/>
    <sl:bookmarkOf rdf:resource="https://twitter.com/linkedktk/status/1238031736522620928?s=20"/>
  </rdf:Description>
  <rdf:Description rdf:about="gilda">
    <dc:title>Gilda</dc:title>
    <sl:creationDate>2020-03-22</sl:creationDate>
    <sl:creationTime>2020-03-22T19:08:11Z</sl:creationTime>
    <sl:bookmarkOf rdf:resource="https://en.wikipedia.org/wiki/Gilda"/>
    <sl:tag rdf:resource="&tag;film_americain"/>
  </rdf:Description>
  <rdf:Description rdf:about="_2003_00330_graph_neural_netwo">
    <sl:comment xml:lang="en">reviews the state-of-the-art on the use of GNNs as a model of neural-symbolic computing.</sl:comment>
    <sl:creationTime>2020-03-15T10:39:59Z</sl:creationTime>
    <sl:bookmarkOf rdf:resource="https://arxiv.org/abs/2003.00330"/>
    <sl:creationDate>2020-03-15</sl:creationDate>
    <dc:title>[2003.00330] Graph Neural Networks Meet Neural-Symbolic Computing: A Survey and Perspective</dc:title>
    <sl:tag rdf:resource="&tag;graph_neural_networks"/>
    <sl:tag rdf:resource="&tag;neural_symbolic_computing"/>
    <sl:tag rdf:resource="&tag;survey"/>
  </rdf:Description>
  <rdf:Description rdf:about="au_gabon_une_grotte_pourrait_r">
    <sl:bookmarkOf rdf:resource="https://www.lemonde.fr/afrique/article/2020/03/09/au-gabon-une-grotte-pourrait-reveler-des-secrets-vieux-de-700-ans_6032355_3212.html"/>
    <sl:creationTime>2020-03-09T17:21:00Z</sl:creationTime>
    <sl:creationDate>2020-03-09</sl:creationDate>
    <dc:title>Au Gabon, une grotte pourrait révéler des secrets vieux de 700 ans</dc:title>
    <sl:tag rdf:resource="&tag;gabon"/>
    <sl:tag rdf:resource="&tag;antiquite_africaine"/>
  </rdf:Description>
  <rdf:Description rdf:about="_1905_06088_neural_symbolic_co">
    <sl:tag rdf:resource="&tag;neural_symbolic_computing"/>
    <sl:bookmarkOf rdf:resource="https://arxiv.org/abs/1905.06088"/>
    <dc:title>[1905.06088] Neural-Symbolic Computing: An Effective Methodology for Principled Integration of Machine Learning and Reasoning</dc:title>
    <sl:creationDate>2020-03-15</sl:creationDate>
    <sl:creationTime>2020-03-15T11:06:28Z</sl:creationTime>
  </rdf:Description>
  <rdf:Description rdf:about="martynas_jusevicius_sur_twitter">
    <sl:tag rdf:resource="&tag;nlp_using_knowledge_graphs"/>
    <sl:tag rdf:resource="&tag;entity_linking"/>
    <sl:creationTime>2020-03-13T10:38:03Z</sl:creationTime>
    <sl:creationDate>2020-03-13</sl:creationDate>
    <dc:title>Martynas Jusevicius sur Twitter : "Is there a solution for entity recognition that would use a local #KnowledgeGraph to look for matches? Ideally any SPARQL datasource..."</dc:title>
    <sl:bookmarkOf rdf:resource="https://twitter.com/namedgraph/status/1238387782944460802?s=20"/>
    <sl:tag rdf:resource="&tag;tweet"/>
    <sl:tag rdf:resource="&tag;martynas_jusevicius"/>
  </rdf:Description>
  <rdf:Description rdf:about="_1909_07606_k_bert_enabling_l">
    <sl:creationTime>2020-03-08T22:54:15Z</sl:creationTime>
    <sl:creationDate>2020-03-08</sl:creationDate>
    <sl:tag rdf:resource="&tag;nlp_using_knowledge_graphs"/>
    <sl:bookmarkOf rdf:resource="https://arxiv.org/abs/1909.07606"/>
    <sl:tag rdf:resource="&tag;knowledge_graph_deep_learning"/>
    <sl:tag rdf:resource="&tag;bert"/>
    <sl:tag rdf:resource="&tag;language_model"/>
    <sl:comment>a knowledge-enabled language representation model (K-BERT) with knowledge graphs (KGs), in which triples are injected into the sentences as domain knowledge</sl:comment>
    <dc:title>[1909.07606] K-BERT: Enabling Language Representation with Knowledge Graph</dc:title>
  </rdf:Description>
  <rdf:Description rdf:about="max_little_sur_twitter_causa">
    <sl:tag rdf:resource="&tag;causal_inference"/>
    <dc:title>Max Little sur Twitter : "Causal bootstrapping - a simple way of doing causal inference using arbitrary machine learning algo..."</dc:title>
    <sl:tag rdf:resource="&tag;nn_symbolic_ai_hybridation"/>
    <sl:bookmarkOf rdf:resource="https://twitter.com/MaxALittle/status/1236234054405627905?s=20"/>
    <sl:comment xml:lang="en">&gt; Techniques from causal inference, such as probabilistic causal diagrams and do-calculus, provide powerful (nonparametric) tools for drawing causal inferences from such observational data. However, these techniques are often incompatible with modern, nonparametric machine learning algorithms since they typically require explicit probabilistic models. Here, we develop causal bootstrapping for augmenting classical nonparametric bootstrap resampling with information on the causal relationship between variables</sl:comment>
    <sl:tag rdf:resource="&tag;bayesian_deep_learning"/>
    <sl:tag rdf:resource="&tag;machine_learning"/>
    <sl:creationDate>2020-03-08</sl:creationDate>
    <sl:creationTime>2020-03-08T11:36:48Z</sl:creationTime>
  </rdf:Description>
  <rdf:Description rdf:about="how_taiwan_fended_off_the_coron">
    <dc:title>How Taiwan fended off the coronavirus | WORLD News Group</dc:title>
    <sl:creationDate>2020-03-29</sl:creationDate>
    <sl:creationTime>2020-03-29T16:02:11Z</sl:creationTime>
    <sl:bookmarkOf rdf:resource="https://world.wng.org/2020/03/how_taiwan_fended_off_the_coronavirus"/>
    <sl:tag rdf:resource="&tag;coronavirus"/>
    <sl:tag rdf:resource="&tag;taiwan"/>
  </rdf:Description>
  <rdf:Description rdf:about="_1902_10197_rotate_knowledge_">
    <dc:title xml:lang="en">[1902.10197] RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space</dc:title>
    <sl:creationTime>2020-03-03T13:27:48Z</sl:creationTime>
    <sl:comment>&gt; We study the problem of learning representations of entities and relations in knowledge graphs for predicting missing links.</sl:comment>
    <sl:creationDate>2020-03-03</sl:creationDate>
    <sl:bookmarkOf rdf:resource="https://arxiv.org/abs/1902.10197?"/>
    <sl:tag rdf:resource="&tag;knowledge_graph_embeddings"/>
    <sl:tag rdf:resource="&tag;knowledge_graph_completion"/>
  </rdf:Description>
  <rdf:Description rdf:about="unsupervised_ner_using_bert_h">
    <sl:tag rdf:resource="&tag;named_entity_recognition"/>
    <sl:tag rdf:resource="&tag;bert"/>
    <dc:title>Unsupervised NER using BERT - Hands-on NLP model review - Quora</dc:title>
    <sl:creationDate>2020-03-06</sl:creationDate>
    <sl:creationTime>2020-03-06T00:12:06Z</sl:creationTime>
    <sl:bookmarkOf rdf:resource="https://www.quora.com/q/idpysofgzpanjxuh/Unsupervised-NER-using-BERT?ch=10&amp;share=7b4a15bb"/>
  </rdf:Description>
  <sl:ArxivDoc rdf:about="_1911_02168_coke_contextualiz">
    <sl:arxiv_firstAuthor>

Quan Wang

</sl:arxiv_firstAuthor>
    <sl:arxiv_summary xml:lang="en">Knowledge graph embedding, which projects symbolic entities and relations
into continuous vector spaces, is gaining increasing attention. Previous
methods allow a single static embedding for each entity or relation, ignoring
their intrinsic contextual nature, i.e., entities and relations may appear in
different graph contexts, and accordingly, exhibit different properties. This
work presents Contextualized Knowledge Graph Embedding (CoKE), a novel paradigm
that takes into account such contextual nature, and learns dynamic, flexible,
and fully contextualized entity and relation embeddings. Two types of graph
contexts are studied: edges and paths, both formulated as sequences of entities
and relations. CoKE takes a sequence as input and uses a Transformer encoder to
obtain contextualized representations. These representations are hence
naturally adaptive to the input, capturing contextual meanings of entities and
relations therein. Evaluation on a wide variety of public benchmarks verifies
the superiority of CoKE in link prediction and path query answering. It
performs consistently better than, or at least equally well as current
state-of-the-art in almost every case, in particular offering an absolute
improvement of 21.0% in H@10 on path query answering. Our code is available at
\url{https://github.com/PaddlePaddle/Research/tree/master/KG/CoKE}.</sl:arxiv_summary>
    <sl:arxiv_author>

Haifeng Wang

</sl:arxiv_author>
    <sl:comment xml:lang="en">A method to build contextualized entity and relation embeddings. Entities and relations may appear in different graph contexts. **Edges and paths, both formulated as sequences of entities and relations, are passed as input to a Transformer encoder to learn the contextualized representations..**</sl:comment>
    <sl:tag rdf:resource="&tag;attention_knowledge_graphs"/>
    <sl:tag rdf:resource="&tag;baidu"/>
    <sl:arxiv_author>

Songtai Dai

</sl:arxiv_author>
    <sl:bookmarkOf rdf:resource="https://arxiv.org/abs/1911.02168"/>
    <sl:arxiv_updated>2020-04-04T07:22:20Z</sl:arxiv_updated>
    <sl:arxiv_author>

Quan Wang

</sl:arxiv_author>
    <sl:arxiv_published>2019-11-06T02:27:39Z</sl:arxiv_published>
    <sl:arxiv_num>1911.02168</sl:arxiv_num>
    <sl:creationTime>2020-03-22T17:34:10Z</sl:creationTime>
    <sl:arxiv_author>

Jing Liu

</sl:arxiv_author>
    <sl:tag rdf:resource="&tag;knowledge_graph_embeddings"/>
    <dc:title xml:lang="en">[Wang

2019] CoKE: Contextualized Knowledge Graph Embedding (Arxiv:1911.02168)</dc:title>
    <sl:arxiv_author>

Pingping Huang

</sl:arxiv_author>
    <sl:arxiv_author>

Yong Zhu

</sl:arxiv_author>
    <sl:arxiv_author>

Yajuan Lyu

</sl:arxiv_author>
    <sl:arxiv_title xml:lang="en">CoKE: Contextualized Knowledge Graph Embedding</sl:arxiv_title>
    <sl:creationDate>2020-03-22</sl:creationDate>
    <sl:arxiv_author>

Hua Wu

</sl:arxiv_author>
    <sl:arxiv_author>

Wenbin Jiang

</sl:arxiv_author>
  </sl:ArxivDoc>
  <rdf:Description rdf:about="au_kenya_l%E2%80%99unique_girafe_blanc">
    <sl:tag rdf:resource="&tag;kenya"/>
    <dc:title>Au Kenya, l’unique girafe blanche femelle et son petit tués par des braconniers</dc:title>
    <sl:creationDate>2020-03-11</sl:creationDate>
    <sl:creationTime>2020-03-11T16:33:15Z</sl:creationTime>
    <sl:bookmarkOf rdf:resource="https://www.lemonde.fr/afrique/article/2020/03/11/au-kenya-l-unique-girafe-blanche-femelle-et-son-petit-tues-par-des-braconniers_6032601_3212.html"/>
    <sl:tag rdf:resource="&tag;girafe"/>
    <sl:tag rdf:resource="&tag;l_humanite_merite_de_disparaitre"/>
  </rdf:Description>
  <rdf:Description rdf:about="combining_knowledge_graphs_qui">
    <sl:tag rdf:resource="&tag;combining_knowledge_graphs"/>
    <sl:tag rdf:resource="&tag;attention_knowledge_graphs"/>
    <dc:title>Combining knowledge graphs, quickly and accurately</dc:title>
    <sl:comment xml:lang="en">Entity matching at Amazon: a new [#entity alignment](/tag/entity_alignment) technique that factors in information about the graph in the vicinity of the entity name.&#xD;
&#xD;
[#Graph neural network](/tag/graph_neural_networks) that specifically addresses the problem of **merging multi-type knowledge graphs**. &#xD;
&#xD;
http://127.0.0.1:8080/semanlink/tag/graph_neural_networks.html</sl:comment>
    <sl:tag rdf:resource="&tag;entity_alignment"/>
    <sl:tag rdf:resource="&tag;ai_amazon"/>
    <sl:creationTime>2020-03-19T21:33:27Z</sl:creationTime>
    <sl:bookmarkOf rdf:resource="https://www.amazon.science/blog/combining-knowledge-graphs-quickly-and-accurately"/>
    <sl:creationDate>2020-03-19</sl:creationDate>
    <sl:tag rdf:resource="&tag;graph_neural_networks"/>
  </rdf:Description>
  <rdf:Description rdf:about="cs294_158_sp20_deep_unsupervise">
    <sl:tag rdf:resource="&tag;generative_model"/>
    <sl:tag rdf:resource="&tag;deep_unsupervised_learning"/>
    <sl:tag rdf:resource="&tag;berkeley"/>
    <sl:creationTime>2020-03-08T11:45:16Z</sl:creationTime>
    <sl:bookmarkOf rdf:resource="https://sites.google.com/view/berkeley-cs294-158-sp20/home"/>
    <sl:comment>cover two areas of deep learning in which labeled data is not required: Deep Generative Models and Self-supervised Learning</sl:comment>
    <dc:title>CS294-158-SP20 Deep Unsupervised Learning Spring 2020</dc:title>
    <sl:creationDate>2020-03-08</sl:creationDate>
    <sl:tag rdf:resource="&tag;machine_learning_course"/>
  </rdf:Description>
  <rdf:Description rdf:about="linkeddatahub_atomgraph_s_ope">
    <dc:title>LinkedDataHub - AtomGraph's open-source Knowledge Graph management system</dc:title>
    <sl:tag rdf:resource="&tag;knowledge_graph"/>
    <sl:creationDate>2020-03-05</sl:creationDate>
    <sl:creationTime>2020-03-05T13:08:32Z</sl:creationTime>
    <sl:comment>&gt; It is as easy to use for graph data as WordPress is for web content</sl:comment>
    <sl:bookmarkOf rdf:resource="https://atomgraph.com/blog/finally-a-knowledge-graph-management-system/"/>
  </rdf:Description>
  <rdf:Description rdf:about="mapper_annotated_text_plugin_%7C_">
    <sl:tag rdf:resource="&tag;elasticsearch_annotated_text_field"/>
    <dc:title>Mapper Annotated Text Plugin | Elastic</dc:title>
    <sl:bookmarkOf rdf:resource="https://www.elastic.co/guide/en/elasticsearch/plugins/current/mapper-annotated-text.html"/>
    <sl:creationTime>2020-03-14T11:47:52Z</sl:creationTime>
    <sl:creationDate>2020-03-14</sl:creationDate>
    <sl:comment xml:lang="en">The doc about annotated text fields. See also elastic list:&#xD;
&#xD;
-  &lt;https://discuss.elastic.co/t/can-elasticsearch-handle-long-text/173991/2&gt;&#xD;
- &lt;https://discuss.elastic.co/t/continued-support-for-annotated-text-plugin/218688&gt;</sl:comment>
  </rdf:Description>
  <rdf:Description rdf:about="one_track_minds_using_ai_for_m">
    <sl:bookmarkOf rdf:resource="https://tech.fb.com/one-track-minds-using-ai-for-music-source-separation/"/>
    <sl:creationTime>2020-03-08T12:11:37Z</sl:creationTime>
    <sl:creationDate>2020-03-08</sl:creationDate>
    <dc:title>One-track minds: Using AI for music source separation</dc:title>
    <sl:tag rdf:resource="&tag;ai_facebook"/>
    <sl:tag rdf:resource="&tag;music_source_separation"/>
  </rdf:Description>
  <rdf:Description rdf:about="la_plus_grosse_explosion_jamais">
    <sl:bookmarkOf rdf:resource="https://www.sciencesetavenir.fr/espace/univers/un-trou-noir-responsable-de-la-plus-grosse-explosion-jamais-observee-dans-l-espace-depuis-le-big-bang_142029"/>
    <sl:creationTime>2020-03-01T12:15:45Z</sl:creationTime>
    <sl:creationDate>2020-03-01</sl:creationDate>
    <dc:title>La plus grosse explosion jamais observée depuis le Big Bang</dc:title>
    <sl:comment>L'événement fut si puissant qu'il aurait créé une brèche de la taille de 15 Voies lactées réunies dans le plasma environnant. &#xD;
&#xD;
&gt; L'Univers est un endroit étrange.</sl:comment>
    <sl:tag rdf:resource="&tag;astrophysique"/>
  </rdf:Description>
  <rdf:Description rdf:about="ambiversenlu_a_natural_languag">
    <sl:tag rdf:resource="&tag;nlp_tools"/>
    <sl:tag rdf:resource="&tag;concept_extraction"/>
    <sl:tag rdf:resource="&tag;nlp_using_knowledge_graphs"/>
    <sl:tag rdf:resource="&tag;entity_linking"/>
    <sl:bookmarkOf rdf:resource="https://github.com/ambiverse-nlu/ambiverse-nlu"/>
    <sl:creationTime>2020-03-13T10:30:41Z</sl:creationTime>
    <sl:creationDate>2020-03-13</sl:creationDate>
    <dc:title>AmbiverseNLU: A Natural Language Understanding suite by Max Planck Institute for Informatics</dc:title>
    <sl:tag rdf:resource="&tag;github_project"/>
  </rdf:Description>
  <rdf:Description rdf:about="diy_masks_for_all_could_help_st">
    <sl:tag rdf:resource="&tag;jeremy_howard"/>
    <sl:tag rdf:resource="&tag;diy"/>
    <sl:bookmarkOf rdf:resource="https://www.washingtonpost.com/outlook/2020/03/28/masks-all-coronavirus/"/>
    <sl:creationTime>2020-03-29T10:47:45Z</sl:creationTime>
    <sl:creationDate>2020-03-29</sl:creationDate>
    <dc:title>DIY masks for all could help stop coronavirus - The Washington Post</dc:title>
    <sl:tag rdf:resource="&tag;coronavirus"/>
  </rdf:Description>
  <rdf:Description rdf:about="neuromorphic_spintronics_%7C_natu">
    <sl:tag rdf:resource="&tag;brains_in_silicon"/>
    <sl:creationTime>2020-03-02T19:55:39Z</sl:creationTime>
    <sl:creationDate>2020-03-02</sl:creationDate>
    <dc:title>Neuromorphic spintronics | Nature Electronics</dc:title>
    <sl:bookmarkOf rdf:resource="https://www.nature.com/articles/s41928-019-0360-9.epdf?author_access_token=jtNeSAhVDL6lK9Q0yunEtdRgN0jAjWel9jnR3ZoTv0PgNbthozMob-1_2TKB9x_fHQcXMtfnbJHqU8V34xrEFK_D8iG774ueRc9x-R_k0v1d-2Pjco0iE67uXMbZ8pklVwMI2YTodu1XqKlKXwInjw%3D%3D"/>
    <sl:tag rdf:resource="&tag;julie_grollier"/>
  </rdf:Description>
  <rdf:Description rdf:about="_2003_08271_pre_trained_models">
    <sl:tag rdf:resource="&tag;survey"/>
    <sl:tag rdf:resource="&tag;pre_trained_language_models"/>
    <dc:title>[2003.08271] Pre-trained Models for Natural Language Processing: A Survey</dc:title>
    <sl:bookmarkOf rdf:resource="https://arxiv.org/abs/2003.08271"/>
    <sl:creationTime>2020-03-19T13:34:50Z</sl:creationTime>
    <sl:creationDate>2020-03-19</sl:creationDate>
  </rdf:Description>
  <rdf:Description rdf:about="coronavirus_why_you_must_act_n">
    <dc:title>Coronavirus: Why You Must Act Now - Tomas Pueyo - Medium</dc:title>
    <sl:tag rdf:resource="&tag;coronavirus"/>
    <sl:bookmarkOf rdf:resource="https://medium.com/@tomaspueyo/coronavirus-act-today-or-people-will-die-f4d3d9cd99ca"/>
    <sl:creationTime>2020-03-11T00:43:20Z</sl:creationTime>
    <sl:creationDate>2020-03-11</sl:creationDate>
  </rdf:Description>
  <rdf:Description rdf:about="taiwan_un_modele_dans_la_lutte">
    <dc:title xml:lang="fr">Taïwan, un modèle dans la lutte contre le coronavirus (RFI - 12/03/2020)</dc:title>
    <sl:bookmarkOf rdf:resource="http://www.rfi.fr/fr/asie-pacifique/20200311-ta%C3%AFwan-bonne-gestion-crise-coronavirus"/>
    <sl:creationTime>2020-03-29T15:48:59Z</sl:creationTime>
    <sl:creationDate>2020-03-29</sl:creationDate>
    <sl:tag rdf:resource="&tag;coronavirus"/>
    <sl:tag rdf:resource="&tag;taiwan"/>
  </rdf:Description>
  <rdf:Description rdf:about="_2003_03384_automl_zero_evolv">
    <sl:tag rdf:resource="&tag;automl"/>
    <sl:bookmarkOf rdf:resource="https://arxiv.org/abs/2003.03384"/>
    <sl:comment>&gt; Fun AutoML-Zero experiments: Evolutionary search discovers fundamental ML algorithms from scratch, e.g., small neural nets with backprop.&#xD;
&gt; Can evolution be the “Master Algorithm”? ;)</sl:comment>
    <sl:creationDate>2020-03-17</sl:creationDate>
    <sl:tag rdf:resource="&tag;evolutionary_algorithm"/>
    <sl:creationTime>2020-03-17T21:57:40Z</sl:creationTime>
    <dc:title>[2003.03384] AutoML-Zero: Evolving Machine Learning Algorithms From Scratch</dc:title>
    <sl:tag rdf:resource="&tag;backpropagation_vs_biology"/>
    <sl:tag rdf:resource="&tag;quoc_le"/>
  </rdf:Description>
  <sl:ArxivDoc rdf:about="_1909_03193_kg_bert_bert_for_">
    <sl:arxiv_published>2019-09-07T06:09:25Z</sl:arxiv_published>
    <sl:comment>Pre-trained language models for knowledge graph completion. **Triples are treated as textual sequences**. (Hum, j'ai déjà vu ça quelque part)</sl:comment>
    <dc:title xml:lang="en">[Yao

2019] KG-BERT: BERT for Knowledge Graph Completion (Arxiv:1909.03193)</dc:title>
    <sl:arxiv_author>

Chengsheng Mao

</sl:arxiv_author>
    <sl:tag rdf:resource="&tag;knowledge_graph"/>
    <sl:arxiv_summary xml:lang="en">Knowledge graphs are important resources for many artificial intelligence
tasks but often suffer from incompleteness. In this work, we propose to use
pre-trained language models for knowledge graph completion. We treat triples in
knowledge graphs as textual sequences and propose a novel framework named
Knowledge Graph Bidirectional Encoder Representations from Transformer
(KG-BERT) to model these triples. Our method takes entity and relation
descriptions of a triple as input and computes scoring function of the triple
with the KG-BERT language model. Experimental results on multiple benchmark
knowledge graphs show that our method can achieve state-of-the-art performance
in triple classification, link prediction and relation prediction tasks.</sl:arxiv_summary>
    <sl:arxiv_num>1909.03193</sl:arxiv_num>
    <sl:bookmarkOf rdf:resource="https://arxiv.org/abs/1909.03193"/>
    <sl:tag rdf:resource="&tag;bert"/>
    <sl:arxiv_updated>2019-09-11T06:03:30Z</sl:arxiv_updated>
    <sl:arxiv_author>

Yuan Luo

</sl:arxiv_author>
    <sl:creationTime>2020-03-22T18:56:43Z</sl:creationTime>
    <sl:tag rdf:resource="&tag;pre_trained_language_models"/>
    <sl:arxiv_author>

Liang Yao

</sl:arxiv_author>
    <sl:arxiv_firstAuthor>

Liang Yao

</sl:arxiv_firstAuthor>
    <sl:tag rdf:resource="&tag;attention_is_all_you_need"/>
    <sl:tag rdf:resource="&tag;knowledge_graph_completion"/>
    <sl:creationDate>2020-03-22</sl:creationDate>
    <sl:arxiv_title xml:lang="en">KG-BERT: BERT for Knowledge Graph Completion</sl:arxiv_title>
  </sl:ArxivDoc>
  <rdf:Description rdf:about="chengkai_li_sur_twitter_link">
    <sl:tag rdf:resource="&tag;knowledge_graph_embeddings"/>
    <sl:bookmarkOf rdf:resource="https://twitter.com/Chengkai_Li/status/1243342637089898496"/>
    <sl:creationDate>2020-03-29</sl:creationDate>
    <sl:tag rdf:resource="&tag;link_prediction"/>
    <sl:creationTime>2020-03-29T11:40:20Z</sl:creationTime>
    <dc:title>Chengkai Li sur Twitter : "Link prediction methods on knowledge graphs don't work..."</dc:title>
    <sl:tag rdf:resource="&tag;knowledge_graph_completion"/>
    <sl:tag rdf:resource="&tag;tweet"/>
  </rdf:Description>
  <rdf:Description rdf:about="chaitanya_joshi_sur_twitter_">
    <sl:tag rdf:resource="&tag;graph_neural_networks"/>
    <sl:tag rdf:resource="&tag;tweet"/>
    <sl:bookmarkOf rdf:resource="https://twitter.com/chaitjo/status/1233220586358181888"/>
    <sl:mainDoc>
      <rdf:Description rdf:about="transformers_are_graph_neural_n">
        <sl:tag rdf:resource="&tag;attention_is_all_you_need"/>
        <sl:tag rdf:resource="&tag;graph_neural_networks"/>
        <sl:tag rdf:resource="&tag;graph_attention_networks"/>
        <sl:creationTime>2020-03-01T02:28:59Z</sl:creationTime>
        <dc:title>Transformers are Graph Neural Networks | NTU Graph Deep Learning Lab</dc:title>
        <sl:creationDate>2020-03-01</sl:creationDate>
        <sl:comment xml:lang="en">&gt; The key idea: Sentences are fully-connected graphs of words, and Transformers are very similar to Graph Attention Networks (GATs) which use multi-head attention to aggregate features from their neighborhood nodes (i.e., words).&#xD;
[ twitter](https://twitter.com/chaitjo/status/1233220586358181888)</sl:comment>
        <sl:bookmarkOf rdf:resource="https://graphdeeplearning.github.io/post/transformers-are-gnns/"/>
      </rdf:Description>
    </sl:mainDoc>
    <sl:tag rdf:resource="&tag;graph_attention_networks"/>
    <sl:creationTime>2020-03-01T03:17:11Z</sl:creationTime>
    <sl:creationDate>2020-03-01</sl:creationDate>
    <sl:comment xml:lang="fr">[about this blog post](/doc/2020/03/transformers_are_graph_neural_n)</sl:comment>
    <sl:tag rdf:resource="&tag;attention_is_all_you_need"/>
    <dc:title>Chaitanya Joshi sur Twitter : "Excited to share a blog post on the connection between #Transformers for NLP and #GraphNeuralNetworks"</dc:title>
  </rdf:Description>
  <rdf:Description rdf:about="google_and_http">
    <sl:tag rdf:resource="&tag;abuse_of_power"/>
    <sl:tag rdf:resource="&tag;http"/>
    <sl:tag rdf:resource="&tag;google"/>
    <sl:comment>&gt; Google’s not secure message means this: “Google tried to take control of the open web and this site said no.”</sl:comment>
    <sl:bookmarkOf rdf:resource="http://this.how/googleAndHttp/"/>
    <sl:creationTime>2020-03-08T22:48:47Z</sl:creationTime>
    <sl:creationDate>2020-03-08</sl:creationDate>
    <dc:title>Google and HTTP</dc:title>
    <sl:tag rdf:resource="&tag;dave_winer"/>
  </rdf:Description>
  <rdf:Description rdf:about="_2003_02320_knowledge_graphs">
    <sl:comment xml:lang="en">Draws together many topics &amp; perspectives regarding Knowledge Graphs. 18 co-authors, lead by Aidan Hogan. (Regarding language models for embedding, they refer to [Wang et al. Knowledge Graph Embedding: A Survey of Approaches and Applications](/doc/2019/05/knowledge_graph_embedding_a_su))</sl:comment>
    <sl:tag rdf:resource="&tag;survey"/>
    <sl:bookmarkOf rdf:resource="https://arxiv.org/abs/2003.02320"/>
    <sl:creationTime>2020-03-07T09:20:34Z</sl:creationTime>
    <sl:creationDate>2020-03-07</sl:creationDate>
    <dc:title>[2003.02320] Knowledge Graphs</dc:title>
    <sl:tag rdf:resource="&tag;knowledge_graph"/>
    <sl:tag rdf:resource="&tag;axel_polleres"/>
    <sl:tag rdf:resource="&tag;aidan_hogan"/>
  </rdf:Description>
</rdf:RDF>
