<!DOCTYPE rdf:RDF [
  <!ENTITY skos 'http://www.w3.org/2004/02/skos/core#'>
  <!ENTITY sl 'http://www.semanlink.net/2001/00/semanlink-schema#'>
  <!ENTITY tag 'http://www.semanlink.net/tag/'>
  <!ENTITY rdf 'http://www.w3.org/1999/02/22-rdf-syntax-ns#'>
  <!ENTITY dc 'http://purl.org/dc/elements/1.1/'>]>
<rdf:RDF
    xmlns:rdf="&rdf;"
    xmlns:dc="&dc;"
    xmlns:skos="&skos;"
    xmlns:sl="&sl;"
    xmlns:tag="&tag;">
  <rdf:Description rdf:about="_1909_03193_kg_bert_bert_for_">
    <sl:arxiv_num>1909.03193</sl:arxiv_num>
    <sl:tag rdf:resource="&tag;arxiv_doc"/>
    <sl:creationTime>2020-03-22T18:56:43Z</sl:creationTime>
    <sl:arxiv_published>2019-09-07T06:09:25Z</sl:arxiv_published>
    <sl:arxiv_author>Liang Yao</sl:arxiv_author>
    <sl:arxiv_title xml:lang="en">KG-BERT: BERT for Knowledge Graph Completion</sl:arxiv_title>
    <sl:arxiv_author>Yuan Luo</sl:arxiv_author>
    <dc:title xml:lang="en">[1909.03193] KG-BERT: BERT for Knowledge Graph Completion</dc:title>
    <sl:tag rdf:resource="&tag;attention_is_all_you_need"/>
    <sl:tag rdf:resource="&tag;pre_trained_language_models"/>
    <sl:tag rdf:resource="&tag;bert"/>
    <sl:tag rdf:resource="&tag;knowledge_graph"/>
    <sl:arxiv_summary xml:lang="en">Knowledge graphs are important resources for many artificial intelligence
tasks but often suffer from incompleteness. In this work, we propose to use
pre-trained language models for knowledge graph completion. We treat triples in
knowledge graphs as textual sequences and propose a novel framework named
Knowledge Graph Bidirectional Encoder Representations from Transformer
(KG-BERT) to model these triples. Our method takes entity and relation
descriptions of a triple as input and computes scoring function of the triple
with the KG-BERT language model. Experimental results on multiple benchmark
knowledge graphs show that our method can achieve state-of-the-art performance
in triple classification, link prediction and relation prediction tasks.</sl:arxiv_summary>
    <sl:arxiv_firstAuthor>Liang Yao</sl:arxiv_firstAuthor>
    <sl:comment>Pre-trained language models for knowledge graph completion. **Triples are treated as textual sequences**. (Hum, j'ai déjà vu ça quelque part)</sl:comment>
    <sl:creationDate>2020-03-22</sl:creationDate>
    <sl:tag rdf:resource="&tag;knowledge_graph_completion"/>
    <sl:bookmarkOf rdf:resource="https://arxiv.org/abs/1909.03193"/>
    <sl:arxiv_author>Chengsheng Mao</sl:arxiv_author>
    <sl:arxiv_updated>2019-09-11T06:03:30Z</sl:arxiv_updated>
  </rdf:Description>
  <rdf:Description rdf:about="bert_elmo_gpt_2_how_contex">
    <sl:tag rdf:resource="&tag;emnlp_2019"/>
    <sl:creationDate>2020-03-28</sl:creationDate>
    <sl:creationTime>2020-03-28T10:33:17Z</sl:creationTime>
    <sl:bookmarkOf rdf:resource="http://ai.stanford.edu/blog/contextual/"/>
    <dc:title>BERT, ELMo, &amp; GPT-2: How Contextual are Contextualized Word Representations? | SAIL Blog</dc:title>
    <sl:tag rdf:resource="&tag;contextualised_word_representations"/>
    <sl:tag rdf:resource="&tag;nlp_stanford"/>
    <sl:tag rdf:resource="&tag;bertology"/>
  </rdf:Description>
  <rdf:Description rdf:about="linkeddatahub_atomgraph_s_ope">
    <dc:title>LinkedDataHub - AtomGraph's open-source Knowledge Graph management system</dc:title>
    <sl:tag rdf:resource="&tag;knowledge_graph"/>
    <sl:creationDate>2020-03-05</sl:creationDate>
    <sl:creationTime>2020-03-05T13:08:32Z</sl:creationTime>
    <sl:comment>&gt; It is as easy to use for graph data as WordPress is for web content</sl:comment>
    <sl:bookmarkOf rdf:resource="https://atomgraph.com/blog/finally-a-knowledge-graph-management-system/"/>
  </rdf:Description>
  <rdf:Description rdf:about="coronavirus_why_you_must_act_n">
    <dc:title>Coronavirus: Why You Must Act Now - Tomas Pueyo - Medium</dc:title>
    <sl:tag rdf:resource="&tag;coronavirus"/>
    <sl:bookmarkOf rdf:resource="https://medium.com/@tomaspueyo/coronavirus-act-today-or-people-will-die-f4d3d9cd99ca"/>
    <sl:creationTime>2020-03-11T00:43:20Z</sl:creationTime>
    <sl:creationDate>2020-03-11</sl:creationDate>
  </rdf:Description>
  <rdf:Description rdf:about="_2003_03384_automl_zero_evolv">
    <sl:comment>&gt; Fun AutoML-Zero experiments: Evolutionary search discovers fundamental ML algorithms from scratch, e.g., small neural nets with backprop.&#xD;
&gt; Can evolution be the “Master Algorithm”? ;)</sl:comment>
    <sl:tag rdf:resource="&tag;evolutionary_algorithm"/>
    <sl:arxiv_author>David R. So</sl:arxiv_author>
    <sl:tag rdf:resource="&tag;quoc_le"/>
    <sl:arxiv_author>Esteban Real</sl:arxiv_author>
    <sl:arxiv_author>Chen Liang</sl:arxiv_author>
    <sl:arxiv_title xml:lang="en">AutoML-Zero: Evolving Machine Learning Algorithms From Scratch</sl:arxiv_title>
    <sl:arxiv_firstAuthor>Esteban Real</sl:arxiv_firstAuthor>
    <sl:tag rdf:resource="&tag;backpropagation_vs_biology"/>
    <sl:arxiv_updated>2020-03-06T19:00:04Z</sl:arxiv_updated>
    <sl:tag rdf:resource="&tag;automl"/>
    <sl:tag rdf:resource="&tag;arxiv_doc"/>
    <sl:creationTime>2020-03-17T21:57:40Z</sl:creationTime>
    <sl:arxiv_num>2003.03384</sl:arxiv_num>
    <sl:arxiv_published>2020-03-06T19:00:04Z</sl:arxiv_published>
    <dc:title xml:lang="en">[2003.03384] AutoML-Zero: Evolving Machine Learning Algorithms From Scratch</dc:title>
    <sl:creationDate>2020-03-17</sl:creationDate>
    <sl:arxiv_author>Quoc V. Le</sl:arxiv_author>
    <sl:arxiv_summary xml:lang="en">Machine learning research has advanced in multiple aspects, including model
structures and learning methods. The effort to automate such research, known as
AutoML, has also made significant progress. However, this progress has largely
focused on the architecture of neural networks, where it has relied on
sophisticated expert-designed layers as building blocks---or similarly
restrictive search spaces. Our goal is to show that AutoML can go further: it
is possible today to automatically discover complete machine learning
algorithms just using basic mathematical operations as building blocks. We
demonstrate this by introducing a novel framework that significantly reduces
human bias through a generic search space. Despite the vastness of this space,
evolutionary search can still discover two-layer neural networks trained by
backpropagation. These simple neural networks can then be surpassed by evolving
directly on tasks of interest, e.g. CIFAR-10 variants, where modern techniques
emerge in the top algorithms, such as bilinear interactions, normalized
gradients, and weight averaging. Moreover, evolution adapts algorithms to
different task types: e.g., dropout-like techniques appear when little data is
available. We believe these preliminary successes in discovering machine
learning algorithms from scratch indicate a promising new direction for the
field.</sl:arxiv_summary>
    <sl:bookmarkOf rdf:resource="https://arxiv.org/abs/2003.03384"/>
  </rdf:Description>
  <rdf:Description rdf:about="au_kenya_l%E2%80%99unique_girafe_blanc">
    <sl:tag rdf:resource="&tag;kenya"/>
    <dc:title>Au Kenya, l’unique girafe blanche femelle et son petit tués par des braconniers</dc:title>
    <sl:creationDate>2020-03-11</sl:creationDate>
    <sl:creationTime>2020-03-11T16:33:15Z</sl:creationTime>
    <sl:bookmarkOf rdf:resource="https://www.lemonde.fr/afrique/article/2020/03/11/au-kenya-l-unique-girafe-blanche-femelle-et-son-petit-tues-par-des-braconniers_6032601_3212.html"/>
    <sl:tag rdf:resource="&tag;girafe"/>
    <sl:tag rdf:resource="&tag;l_humanite_merite_de_disparaitre"/>
  </rdf:Description>
  <rdf:Description rdf:about="one_track_minds_using_ai_for_m">
    <sl:bookmarkOf rdf:resource="https://tech.fb.com/one-track-minds-using-ai-for-music-source-separation/"/>
    <sl:creationTime>2020-03-08T12:11:37Z</sl:creationTime>
    <sl:creationDate>2020-03-08</sl:creationDate>
    <dc:title>One-track minds: Using AI for music source separation</dc:title>
    <sl:tag rdf:resource="&tag;ai_facebook"/>
    <sl:tag rdf:resource="&tag;music_source_separation"/>
  </rdf:Description>
  <rdf:Description rdf:about="combining_knowledge_graphs_qui">
    <sl:tag rdf:resource="&tag;entity_alignment"/>
    <sl:tag rdf:resource="&tag;combining_knowledge_graphs"/>
    <sl:bookmarkOf rdf:resource="https://www.amazon.science/blog/combining-knowledge-graphs-quickly-and-accurately"/>
    <sl:tag rdf:resource="&tag;attention_knowledge_graphs"/>
    <sl:creationTime>2020-03-19T21:33:27Z</sl:creationTime>
    <sl:tag rdf:resource="&tag;ai_amazon"/>
    <dc:title>Combining knowledge graphs, quickly and accurately</dc:title>
    <sl:creationDate>2020-03-19</sl:creationDate>
    <sl:tag rdf:resource="&tag;graph_neural_networks"/>
    <sl:comment xml:lang="en">Entity matching at Amazon: a new [#entity alignment](/tag/entity_alignment) technique that factors in information about the graph in the vicinity of the entity name.&#xD;
&#xD;
[#Graph neural network](/tag/graph_neural_networks) that specifically addresses the problem of **merging multi-type knowledge graphs**. &#xD;
&#xD;
http://127.0.0.1:8080/semanlink/tag/graph_neural_networks.html</sl:comment>
  </rdf:Description>
  <rdf:Description rdf:about="_2003_02320_knowledge_graphs">
    <sl:arxiv_author>Lukas Schmelzeisen</sl:arxiv_author>
    <sl:arxiv_author>Claudia d'Amato</sl:arxiv_author>
    <sl:tag rdf:resource="&tag;arxiv_doc"/>
    <sl:comment xml:lang="en">Draws together many topics &amp; perspectives regarding Knowledge Graphs. 18 co-authors, lead by Aidan Hogan. (Regarding language models for embedding, they refer to [Wang et al. Knowledge Graph Embedding: A Survey of Approaches and Applications](/doc/2019/05/knowledge_graph_embedding_a_su))</sl:comment>
    <sl:arxiv_published>2020-03-04T20:20:32Z</sl:arxiv_published>
    <sl:tag rdf:resource="&tag;survey"/>
    <sl:arxiv_firstAuthor>Aidan Hogan</sl:arxiv_firstAuthor>
    <sl:arxiv_author>Steffen Staab</sl:arxiv_author>
    <sl:bookmarkOf rdf:resource="https://arxiv.org/abs/2003.02320"/>
    <sl:arxiv_author>José Emilio Labra Gayo</sl:arxiv_author>
    <sl:arxiv_author>Gerard de Melo</sl:arxiv_author>
    <sl:tag rdf:resource="&tag;knowledge_graph"/>
    <sl:arxiv_author>Sebastian Neumaier</sl:arxiv_author>
    <sl:tag rdf:resource="&tag;axel_polleres"/>
    <sl:arxiv_author>Juan Sequeda</sl:arxiv_author>
    <sl:arxiv_author>Anisa Rula</sl:arxiv_author>
    <sl:tag rdf:resource="&tag;aidan_hogan"/>
    <sl:arxiv_author>Claudio Gutierrez</sl:arxiv_author>
    <sl:arxiv_author>Sabbir M. Rashid</sl:arxiv_author>
    <sl:arxiv_author>Antoine Zimmermann</sl:arxiv_author>
    <sl:creationDate>2020-03-07</sl:creationDate>
    <sl:arxiv_num>2003.02320</sl:arxiv_num>
    <sl:arxiv_author>Roberto Navigli</sl:arxiv_author>
    <sl:arxiv_author>Axel-Cyrille Ngonga Ngomo</sl:arxiv_author>
    <dc:title xml:lang="en">[2003.02320] Knowledge Graphs</dc:title>
    <sl:arxiv_author>Axel Polleres</sl:arxiv_author>
    <sl:arxiv_updated>2020-04-17T00:07:00Z</sl:arxiv_updated>
    <sl:arxiv_summary xml:lang="en">In this paper we provide a comprehensive introduction to knowledge graphs,
which have recently garnered significant attention from both industry and
academia in scenarios that require exploiting diverse, dynamic, large-scale
collections of data. After a general introduction, we motivate and contrast
various graph-based data models and query languages that are used for knowledge
graphs. We discuss the roles of schema, identity, and context in knowledge
graphs. We explain how knowledge can be represented and extracted using a
combination of deductive and inductive techniques. We summarise methods for the
creation, enrichment, quality assessment, refinement, and publication of
knowledge graphs. We provide an overview of prominent open knowledge graphs and
enterprise knowledge graphs, their applications, and how they use the
aforementioned techniques. We conclude with high-level future research
directions for knowledge graphs.</sl:arxiv_summary>
    <sl:arxiv_author>Eva Blomqvist</sl:arxiv_author>
    <sl:arxiv_title xml:lang="en">Knowledge Graphs</sl:arxiv_title>
    <sl:creationTime>2020-03-07T09:20:34Z</sl:creationTime>
    <sl:arxiv_author>Aidan Hogan</sl:arxiv_author>
    <sl:arxiv_author>Michael Cochez</sl:arxiv_author>
    <sl:arxiv_author>Sabrina Kirrane</sl:arxiv_author>
  </rdf:Description>
  <rdf:Description rdf:about="mapper_annotated_text_plugin_%7C_">
    <sl:tag rdf:resource="&tag;elasticsearch_annotated_text_field"/>
    <dc:title>Mapper Annotated Text Plugin | Elastic</dc:title>
    <sl:bookmarkOf rdf:resource="https://www.elastic.co/guide/en/elasticsearch/plugins/current/mapper-annotated-text.html"/>
    <sl:creationTime>2020-03-14T11:47:52Z</sl:creationTime>
    <sl:creationDate>2020-03-14</sl:creationDate>
    <sl:comment xml:lang="en">The doc about annotated text fields. See also elastic list:&#xD;
&#xD;
-  &lt;https://discuss.elastic.co/t/can-elasticsearch-handle-long-text/173991/2&gt;&#xD;
- &lt;https://discuss.elastic.co/t/continued-support-for-annotated-text-plugin/218688&gt;</sl:comment>
  </rdf:Description>
  <rdf:Description rdf:about="_1909_07606_k_bert_enabling_l">
    <sl:bookmarkOf rdf:resource="https://arxiv.org/abs/1909.07606"/>
    <sl:arxiv_author>Haotang Deng</sl:arxiv_author>
    <sl:arxiv_summary xml:lang="en">Pre-trained language representation models, such as BERT, capture a general
language representation from large-scale corpora, but lack domain-specific
knowledge. When reading a domain text, experts make inferences with relevant
knowledge. For machines to achieve this capability, we propose a
knowledge-enabled language representation model (K-BERT) with knowledge graphs
(KGs), in which triples are injected into the sentences as domain knowledge.
However, too much knowledge incorporation may divert the sentence from its
correct meaning, which is called knowledge noise (KN) issue. To overcome KN,
K-BERT introduces soft-position and visible matrix to limit the impact of
knowledge. K-BERT can easily inject domain knowledge into the models by
equipped with a KG without pre-training by-self because it is capable of
loading model parameters from the pre-trained BERT. Our investigation reveals
promising results in twelve NLP tasks. Especially in domain-specific tasks
(including finance, law, and medicine), K-BERT significantly outperforms BERT,
which demonstrates that K-BERT is an excellent choice for solving the
knowledge-driven problems that require experts.</sl:arxiv_summary>
    <sl:arxiv_author>Zhiruo Wang</sl:arxiv_author>
    <sl:arxiv_author>Peng Zhou</sl:arxiv_author>
    <sl:tag rdf:resource="&tag;bert"/>
    <sl:arxiv_num>1909.07606</sl:arxiv_num>
    <sl:arxiv_updated>2019-09-17T06:16:04Z</sl:arxiv_updated>
    <sl:arxiv_title xml:lang="en">K-BERT: Enabling Language Representation with Knowledge Graph</sl:arxiv_title>
    <sl:comment>a knowledge-enabled language representation model (K-BERT) with knowledge graphs (KGs), in which triples are injected into the sentences as domain knowledge</sl:comment>
    <sl:arxiv_author>Zhe Zhao</sl:arxiv_author>
    <sl:tag rdf:resource="&tag;knowledge_graph_deep_learning"/>
    <sl:tag rdf:resource="&tag;language_model"/>
    <sl:creationDate>2020-03-08</sl:creationDate>
    <sl:tag rdf:resource="&tag;nlp_using_knowledge_graphs"/>
    <sl:arxiv_author>Weijie Liu</sl:arxiv_author>
    <dc:title xml:lang="en">[1909.07606] K-BERT: Enabling Language Representation with Knowledge Graph</dc:title>
    <sl:tag rdf:resource="&tag;arxiv_doc"/>
    <sl:arxiv_author>Ping Wang</sl:arxiv_author>
    <sl:creationTime>2020-03-08T22:54:15Z</sl:creationTime>
    <sl:arxiv_published>2019-09-17T06:16:04Z</sl:arxiv_published>
    <sl:arxiv_author>Qi Ju</sl:arxiv_author>
    <sl:arxiv_firstAuthor>Weijie Liu</sl:arxiv_firstAuthor>
  </rdf:Description>
  <rdf:Description rdf:about="how_taiwan_fended_off_the_coron">
    <dc:title>How Taiwan fended off the coronavirus | WORLD News Group</dc:title>
    <sl:creationDate>2020-03-29</sl:creationDate>
    <sl:creationTime>2020-03-29T16:02:11Z</sl:creationTime>
    <sl:bookmarkOf rdf:resource="https://world.wng.org/2020/03/how_taiwan_fended_off_the_coronavirus"/>
    <sl:tag rdf:resource="&tag;coronavirus"/>
    <sl:tag rdf:resource="&tag;taiwan"/>
  </rdf:Description>
  <rdf:Description rdf:about="ambiversenlu_a_natural_languag">
    <sl:tag rdf:resource="&tag;nlp_tools"/>
    <sl:tag rdf:resource="&tag;concept_extraction"/>
    <sl:tag rdf:resource="&tag;nlp_using_knowledge_graphs"/>
    <sl:tag rdf:resource="&tag;entity_linking"/>
    <sl:bookmarkOf rdf:resource="https://github.com/ambiverse-nlu/ambiverse-nlu"/>
    <sl:creationTime>2020-03-13T10:30:41Z</sl:creationTime>
    <sl:creationDate>2020-03-13</sl:creationDate>
    <dc:title>AmbiverseNLU: A Natural Language Understanding suite by Max Planck Institute for Informatics</dc:title>
    <sl:tag rdf:resource="&tag;github_project"/>
  </rdf:Description>
  <rdf:Description rdf:about="_1905_06088_neural_symbolic_co">
    <sl:arxiv_author>Artur d'Avila Garcez</sl:arxiv_author>
    <sl:arxiv_published>2019-05-15T11:00:48Z</sl:arxiv_published>
    <sl:arxiv_num>1905.06088</sl:arxiv_num>
    <sl:creationTime>2020-03-15T11:06:28Z</sl:creationTime>
    <sl:arxiv_summary xml:lang="en">Current advances in Artificial Intelligence and machine learning in general,
and deep learning in particular have reached unprecedented impact not only
across research communities, but also over popular media channels. However,
concerns about interpretability and accountability of AI have been raised by
influential thinkers. In spite of the recent impact of AI, several works have
identified the need for principled knowledge representation and reasoning
mechanisms integrated with deep learning-based systems to provide sound and
explainable models for such systems. Neural-symbolic computing aims at
integrating, as foreseen by Valiant, two most fundamental cognitive abilities:
the ability to learn from the environment, and the ability to reason from what
has been learned. Neural-symbolic computing has been an active topic of
research for many years, reconciling the advantages of robust learning in
neural networks and reasoning and interpretability of symbolic representation.
In this paper, we survey recent accomplishments of neural-symbolic computing as
a principled methodology for integrated machine learning and reasoning. We
illustrate the effectiveness of the approach by outlining the main
characteristics of the methodology: principled integration of neural learning
with symbolic knowledge representation and reasoning allowing for the
construction of explainable AI systems. The insights provided by
neural-symbolic computing shed new light on the increasingly prominent need for
interpretable and accountable AI systems.</sl:arxiv_summary>
    <sl:arxiv_author>Michael Spranger</sl:arxiv_author>
    <sl:tag rdf:resource="&tag;neural_symbolic_computing"/>
    <sl:arxiv_title xml:lang="en">Neural-Symbolic Computing: An Effective Methodology for Principled Integration of Machine Learning and Reasoning</sl:arxiv_title>
    <sl:arxiv_author>Luis C. Lamb</sl:arxiv_author>
    <sl:bookmarkOf rdf:resource="https://arxiv.org/abs/1905.06088"/>
    <sl:arxiv_author>Marco Gori</sl:arxiv_author>
    <sl:tag rdf:resource="&tag;arxiv_doc"/>
    <sl:arxiv_firstAuthor>Artur d'Avila Garcez</sl:arxiv_firstAuthor>
    <sl:creationDate>2020-03-15</sl:creationDate>
    <sl:arxiv_author>Son N. Tran</sl:arxiv_author>
    <sl:arxiv_author>Luciano Serafini</sl:arxiv_author>
    <dc:title xml:lang="en">[1905.06088] Neural-Symbolic Computing: An Effective Methodology for Principled Integration of Machine Learning and Reasoning</dc:title>
    <sl:arxiv_updated>2019-05-15T11:00:48Z</sl:arxiv_updated>
  </rdf:Description>
  <rdf:Description rdf:about="max_little_sur_twitter_causa">
    <sl:tag rdf:resource="&tag;causal_inference"/>
    <dc:title>Max Little sur Twitter : "Causal bootstrapping - a simple way of doing causal inference using arbitrary machine learning algo..."</dc:title>
    <sl:tag rdf:resource="&tag;nn_symbolic_ai_hybridation"/>
    <sl:bookmarkOf rdf:resource="https://twitter.com/MaxALittle/status/1236234054405627905?s=20"/>
    <sl:comment xml:lang="en">&gt; Techniques from causal inference, such as probabilistic causal diagrams and do-calculus, provide powerful (nonparametric) tools for drawing causal inferences from such observational data. However, these techniques are often incompatible with modern, nonparametric machine learning algorithms since they typically require explicit probabilistic models. Here, we develop causal bootstrapping for augmenting classical nonparametric bootstrap resampling with information on the causal relationship between variables</sl:comment>
    <sl:tag rdf:resource="&tag;bayesian_deep_learning"/>
    <sl:tag rdf:resource="&tag;machine_learning"/>
    <sl:creationDate>2020-03-08</sl:creationDate>
    <sl:creationTime>2020-03-08T11:36:48Z</sl:creationTime>
  </rdf:Description>
  <rdf:Description rdf:about="chengkai_li_sur_twitter_link">
    <sl:tag rdf:resource="&tag;knowledge_graph_embeddings"/>
    <sl:bookmarkOf rdf:resource="https://twitter.com/Chengkai_Li/status/1243342637089898496"/>
    <sl:creationDate>2020-03-29</sl:creationDate>
    <sl:tag rdf:resource="&tag;link_prediction"/>
    <sl:creationTime>2020-03-29T11:40:20Z</sl:creationTime>
    <dc:title>Chengkai Li sur Twitter : "Link prediction methods on knowledge graphs don't work..."</dc:title>
    <sl:tag rdf:resource="&tag;knowledge_graph_completion"/>
    <sl:tag rdf:resource="&tag;tweet"/>
  </rdf:Description>
  <rdf:Description rdf:about="_1902_10197_rotate_knowledge_">
    <sl:arxiv_published>2019-02-26T20:15:09Z</sl:arxiv_published>
    <sl:creationTime>2020-03-03T13:27:48Z</sl:creationTime>
    <sl:arxiv_title xml:lang="en">RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space</sl:arxiv_title>
    <sl:bookmarkOf rdf:resource="https://arxiv.org/abs/1902.10197?"/>
    <sl:tag rdf:resource="&tag;knowledge_graph_completion"/>
    <sl:tag rdf:resource="&tag;arxiv_doc"/>
    <sl:arxiv_num>1902.10197</sl:arxiv_num>
    <sl:creationDate>2020-03-03</sl:creationDate>
    <sl:comment>&gt; We study the problem of learning representations of entities and relations in knowledge graphs for predicting missing links.</sl:comment>
    <dc:title xml:lang="en">[1902.10197] RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space</dc:title>
    <sl:tag rdf:resource="&tag;knowledge_graph_embeddings"/>
    <sl:arxiv_summary xml:lang="en">We study the problem of learning representations of entities and relations in
knowledge graphs for predicting missing links. The success of such a task
heavily relies on the ability of modeling and inferring the patterns of (or
between) the relations. In this paper, we present a new approach for knowledge
graph embedding called RotatE, which is able to model and infer various
relation patterns including: symmetry/antisymmetry, inversion, and composition.
Specifically, the RotatE model defines each relation as a rotation from the
source entity to the target entity in the complex vector space. In addition, we
propose a novel self-adversarial negative sampling technique for efficiently
and effectively training the RotatE model. Experimental results on multiple
benchmark knowledge graphs show that the proposed RotatE model is not only
scalable, but also able to infer and model various relation patterns and
significantly outperform existing state-of-the-art models for link prediction.</sl:arxiv_summary>
    <sl:arxiv_author>Zhiqing Sun</sl:arxiv_author>
    <sl:arxiv_firstAuthor>Zhiqing Sun</sl:arxiv_firstAuthor>
    <sl:arxiv_author>Jian-Yun Nie</sl:arxiv_author>
    <sl:arxiv_author>Jian Tang</sl:arxiv_author>
    <sl:arxiv_updated>2019-02-26T20:15:09Z</sl:arxiv_updated>
    <sl:arxiv_author>Zhi-Hong Deng</sl:arxiv_author>
  </rdf:Description>
  <rdf:Description rdf:about="_1911_02168_coke_contextualiz">
    <sl:creationDate>2020-03-22</sl:creationDate>
    <sl:arxiv_author>Pingping Huang</sl:arxiv_author>
    <sl:arxiv_author>Yajuan Lyu</sl:arxiv_author>
    <dc:title xml:lang="en">[1911.02168] CoKE: Contextualized Knowledge Graph Embedding</dc:title>
    <sl:arxiv_author>Jing Liu</sl:arxiv_author>
    <sl:tag rdf:resource="&tag;knowledge_graph_embeddings"/>
    <sl:arxiv_author>Wenbin Jiang</sl:arxiv_author>
    <sl:arxiv_author>Yong Zhu</sl:arxiv_author>
    <sl:arxiv_author>Songtai Dai</sl:arxiv_author>
    <sl:arxiv_author>Hua Wu</sl:arxiv_author>
    <sl:arxiv_updated>2020-04-04T07:22:20Z</sl:arxiv_updated>
    <sl:tag rdf:resource="&tag;baidu"/>
    <sl:arxiv_firstAuthor>Quan Wang</sl:arxiv_firstAuthor>
    <sl:tag rdf:resource="&tag;attention_knowledge_graphs"/>
    <sl:tag rdf:resource="&tag;arxiv_doc"/>
    <sl:arxiv_author>Haifeng Wang</sl:arxiv_author>
    <sl:arxiv_num>1911.02168</sl:arxiv_num>
    <sl:creationTime>2020-03-22T17:34:10Z</sl:creationTime>
    <sl:bookmarkOf rdf:resource="https://arxiv.org/abs/1911.02168"/>
    <sl:comment xml:lang="en">A method to build contextualized entity and relation embeddings. Entities and relations may appear in different graph contexts. **Edges and paths, both formulated as sequences of entities and relations, are passed as input to a Transformer encoder to learn the contextualized representations..**</sl:comment>
    <sl:arxiv_published>2019-11-06T02:27:39Z</sl:arxiv_published>
    <sl:arxiv_title xml:lang="en">CoKE: Contextualized Knowledge Graph Embedding</sl:arxiv_title>
    <sl:arxiv_author>Quan Wang</sl:arxiv_author>
    <sl:arxiv_summary xml:lang="en">Knowledge graph embedding, which projects symbolic entities and relations
into continuous vector spaces, is gaining increasing attention. Previous
methods allow a single static embedding for each entity or relation, ignoring
their intrinsic contextual nature, i.e., entities and relations may appear in
different graph contexts, and accordingly, exhibit different properties. This
work presents Contextualized Knowledge Graph Embedding (CoKE), a novel paradigm
that takes into account such contextual nature, and learns dynamic, flexible,
and fully contextualized entity and relation embeddings. Two types of graph
contexts are studied: edges and paths, both formulated as sequences of entities
and relations. CoKE takes a sequence as input and uses a Transformer encoder to
obtain contextualized representations. These representations are hence
naturally adaptive to the input, capturing contextual meanings of entities and
relations therein. Evaluation on a wide variety of public benchmarks verifies
the superiority of CoKE in link prediction and path query answering. It
performs consistently better than, or at least equally well as current
state-of-the-art in almost every case, in particular offering an absolute
improvement of 21.0% in H@10 on path query answering. Our code is available at
\url{https://github.com/PaddlePaddle/Research/tree/master/KG/CoKE}.</sl:arxiv_summary>
  </rdf:Description>
  <rdf:Description rdf:about="cs294_158_sp20_deep_unsupervise">
    <sl:tag rdf:resource="&tag;generative_model"/>
    <sl:tag rdf:resource="&tag;deep_unsupervised_learning"/>
    <sl:tag rdf:resource="&tag;berkeley"/>
    <sl:creationTime>2020-03-08T11:45:16Z</sl:creationTime>
    <sl:bookmarkOf rdf:resource="https://sites.google.com/view/berkeley-cs294-158-sp20/home"/>
    <sl:comment>cover two areas of deep learning in which labeled data is not required: Deep Generative Models and Self-supervised Learning</sl:comment>
    <dc:title>CS294-158-SP20 Deep Unsupervised Learning Spring 2020</dc:title>
    <sl:creationDate>2020-03-08</sl:creationDate>
    <sl:tag rdf:resource="&tag;machine_learning_course"/>
  </rdf:Description>
  <rdf:Description rdf:about="unsupervised_ner_using_bert_h">
    <sl:tag rdf:resource="&tag;named_entity_recognition"/>
    <sl:tag rdf:resource="&tag;bert"/>
    <dc:title>Unsupervised NER using BERT - Hands-on NLP model review - Quora</dc:title>
    <sl:creationDate>2020-03-06</sl:creationDate>
    <sl:creationTime>2020-03-06T00:12:06Z</sl:creationTime>
    <sl:bookmarkOf rdf:resource="https://www.quora.com/q/idpysofgzpanjxuh/Unsupervised-NER-using-BERT?ch=10&amp;share=7b4a15bb"/>
  </rdf:Description>
  <rdf:Description rdf:about="gilda">
    <dc:title>Gilda</dc:title>
    <sl:creationDate>2020-03-22</sl:creationDate>
    <sl:creationTime>2020-03-22T19:08:11Z</sl:creationTime>
    <sl:bookmarkOf rdf:resource="https://en.wikipedia.org/wiki/Gilda"/>
    <sl:tag rdf:resource="&tag;film_americain"/>
  </rdf:Description>
  <rdf:Description rdf:about="_2003_00330_graph_neural_netwo">
    <sl:bookmarkOf rdf:resource="https://arxiv.org/abs/2003.00330"/>
    <sl:tag rdf:resource="&tag;survey"/>
    <sl:arxiv_title xml:lang="en">Graph Neural Networks Meet Neural-Symbolic Computing: A Survey and Perspective</sl:arxiv_title>
    <dc:title xml:lang="en">[2003.00330] Graph Neural Networks Meet Neural-Symbolic Computing: A Survey and Perspective</dc:title>
    <sl:arxiv_author>Marco Gori</sl:arxiv_author>
    <sl:arxiv_author>Marcelo Prates</sl:arxiv_author>
    <sl:tag rdf:resource="&tag;graph_neural_networks"/>
    <sl:comment xml:lang="en">reviews the state-of-the-art on the use of GNNs as a model of neural-symbolic computing.</sl:comment>
    <sl:tag rdf:resource="&tag;arxiv_doc"/>
    <sl:arxiv_author>Pedro Avelar</sl:arxiv_author>
    <sl:creationTime>2020-03-15T10:39:59Z</sl:creationTime>
    <sl:arxiv_num>2003.00330</sl:arxiv_num>
    <sl:arxiv_author>Luis Lamb</sl:arxiv_author>
    <sl:arxiv_author>Artur Garcez</sl:arxiv_author>
    <sl:tag rdf:resource="&tag;neural_symbolic_computing"/>
    <sl:arxiv_firstAuthor>Luis Lamb</sl:arxiv_firstAuthor>
    <sl:creationDate>2020-03-15</sl:creationDate>
    <sl:arxiv_author>Moshe Vardi</sl:arxiv_author>
    <sl:arxiv_published>2020-02-29T18:55:13Z</sl:arxiv_published>
    <sl:arxiv_updated>2020-03-11T20:33:01Z</sl:arxiv_updated>
    <sl:arxiv_summary xml:lang="en">Neural-symbolic computing has now become the subject of interest of both
academic and industry research laboratories. Graph Neural Networks (GNN) have
been widely used in relational and symbolic domains, with widespread
application of GNNs in combinatorial optimization, constraint satisfaction,
relational reasoning and other scientific domains. The need for improved
explainability, interpretability and trust of AI systems in general demands
principled methodologies, as suggested by neural-symbolic computing. In this
paper, we review the state-of-the-art on the use of GNNs as a model of
neural-symbolic computing. This includes the application of GNNs in several
domains as well as its relationship to current developments in neural-symbolic
computing.</sl:arxiv_summary>
  </rdf:Description>
  <rdf:Description rdf:about="chaitanya_joshi_sur_twitter_">
    <sl:creationDate>2020-03-01</sl:creationDate>
    <sl:tag rdf:resource="&tag;graph_attention_networks"/>
    <sl:tag rdf:resource="&tag;attention_is_all_you_need"/>
    <sl:bookmarkOf rdf:resource="https://twitter.com/chaitjo/status/1233220586358181888"/>
    <sl:tag rdf:resource="&tag;graph_neural_networks"/>
    <sl:comment xml:lang="fr">[about this blog post](/doc/2020/03/transformers_are_graph_neural_n)</sl:comment>
    <sl:tag rdf:resource="&tag;tweet"/>
    <sl:creationTime>2020-03-01T03:17:11Z</sl:creationTime>
    <dc:title>Chaitanya Joshi sur Twitter : "Excited to share a blog post on the connection between #Transformers for NLP and #GraphNeuralNetworks"</dc:title>
    <sl:mainDoc>
      <rdf:Description rdf:about="transformers_are_graph_neural_n">
        <sl:tag rdf:resource="&tag;attention_is_all_you_need"/>
        <sl:tag rdf:resource="&tag;graph_neural_networks"/>
        <sl:tag rdf:resource="&tag;graph_attention_networks"/>
        <sl:creationTime>2020-03-01T02:28:59Z</sl:creationTime>
        <dc:title>Transformers are Graph Neural Networks | NTU Graph Deep Learning Lab</dc:title>
        <sl:creationDate>2020-03-01</sl:creationDate>
        <sl:comment xml:lang="en">&gt; The key idea: Sentences are fully-connected graphs of words, and Transformers are very similar to Graph Attention Networks (GATs) which use multi-head attention to aggregate features from their neighborhood nodes (i.e., words).&#xD;
[ twitter](https://twitter.com/chaitjo/status/1233220586358181888)</sl:comment>
        <sl:bookmarkOf rdf:resource="https://graphdeeplearning.github.io/post/transformers-are-gnns/"/>
      </rdf:Description>
    </sl:mainDoc>
  </rdf:Description>
  <rdf:Description rdf:about="martynas_jusevicius_sur_twitter">
    <sl:tag rdf:resource="&tag;nlp_using_knowledge_graphs"/>
    <sl:tag rdf:resource="&tag;entity_linking"/>
    <sl:creationTime>2020-03-13T10:38:03Z</sl:creationTime>
    <sl:creationDate>2020-03-13</sl:creationDate>
    <dc:title>Martynas Jusevicius sur Twitter : "Is there a solution for entity recognition that would use a local #KnowledgeGraph to look for matches? Ideally any SPARQL datasource..."</dc:title>
    <sl:bookmarkOf rdf:resource="https://twitter.com/namedgraph/status/1238387782944460802?s=20"/>
    <sl:tag rdf:resource="&tag;tweet"/>
    <sl:tag rdf:resource="&tag;martynas_jusevicius"/>
  </rdf:Description>
  <rdf:Description rdf:about="diy_masks_for_all_could_help_st">
    <sl:tag rdf:resource="&tag;jeremy_howard"/>
    <sl:tag rdf:resource="&tag;diy"/>
    <sl:bookmarkOf rdf:resource="https://www.washingtonpost.com/outlook/2020/03/28/masks-all-coronavirus/"/>
    <sl:creationTime>2020-03-29T10:47:45Z</sl:creationTime>
    <sl:creationDate>2020-03-29</sl:creationDate>
    <dc:title>DIY masks for all could help stop coronavirus - The Washington Post</dc:title>
    <sl:tag rdf:resource="&tag;coronavirus"/>
  </rdf:Description>
  <rdf:Description rdf:about="adrian_gschwend_sur_twitter_">
    <sl:creationTime>2020-03-12T12:38:34Z</sl:creationTime>
    <sl:creationDate>2020-03-12</sl:creationDate>
    <dc:title>Adrian Gschwend sur Twitter : "getting started with RDF and JavaScript!..."</dc:title>
    <sl:tag rdf:resource="&tag;javascript_rdf"/>
    <sl:bookmarkOf rdf:resource="https://twitter.com/linkedktk/status/1238031736522620928?s=20"/>
  </rdf:Description>
  <rdf:Description rdf:about="taiwan_un_modele_dans_la_lutte">
    <dc:title xml:lang="fr">Taïwan, un modèle dans la lutte contre le coronavirus (RFI - 12/03/2020)</dc:title>
    <sl:bookmarkOf rdf:resource="http://www.rfi.fr/fr/asie-pacifique/20200311-ta%C3%AFwan-bonne-gestion-crise-coronavirus"/>
    <sl:creationTime>2020-03-29T15:48:59Z</sl:creationTime>
    <sl:creationDate>2020-03-29</sl:creationDate>
    <sl:tag rdf:resource="&tag;coronavirus"/>
    <sl:tag rdf:resource="&tag;taiwan"/>
  </rdf:Description>
  <rdf:Description rdf:about="au_gabon_une_grotte_pourrait_r">
    <sl:bookmarkOf rdf:resource="https://www.lemonde.fr/afrique/article/2020/03/09/au-gabon-une-grotte-pourrait-reveler-des-secrets-vieux-de-700-ans_6032355_3212.html"/>
    <sl:creationTime>2020-03-09T17:21:00Z</sl:creationTime>
    <sl:creationDate>2020-03-09</sl:creationDate>
    <dc:title>Au Gabon, une grotte pourrait révéler des secrets vieux de 700 ans</dc:title>
    <sl:tag rdf:resource="&tag;gabon"/>
    <sl:tag rdf:resource="&tag;antiquite_africaine"/>
  </rdf:Description>
  <rdf:Description rdf:about="google_and_http">
    <sl:tag rdf:resource="&tag;abuse_of_power"/>
    <sl:tag rdf:resource="&tag;http"/>
    <sl:tag rdf:resource="&tag;google"/>
    <sl:comment>&gt; Google’s not secure message means this: “Google tried to take control of the open web and this site said no.”</sl:comment>
    <sl:bookmarkOf rdf:resource="http://this.how/googleAndHttp/"/>
    <sl:creationTime>2020-03-08T22:48:47Z</sl:creationTime>
    <sl:creationDate>2020-03-08</sl:creationDate>
    <dc:title>Google and HTTP</dc:title>
    <sl:tag rdf:resource="&tag;dave_winer"/>
  </rdf:Description>
  <rdf:Description rdf:about="neuromorphic_spintronics_%7C_natu">
    <sl:tag rdf:resource="&tag;brains_in_silicon"/>
    <sl:creationTime>2020-03-02T19:55:39Z</sl:creationTime>
    <sl:creationDate>2020-03-02</sl:creationDate>
    <dc:title>Neuromorphic spintronics | Nature Electronics</dc:title>
    <sl:bookmarkOf rdf:resource="https://www.nature.com/articles/s41928-019-0360-9.epdf?author_access_token=jtNeSAhVDL6lK9Q0yunEtdRgN0jAjWel9jnR3ZoTv0PgNbthozMob-1_2TKB9x_fHQcXMtfnbJHqU8V34xrEFK_D8iG774ueRc9x-R_k0v1d-2Pjco0iE67uXMbZ8pklVwMI2YTodu1XqKlKXwInjw%3D%3D"/>
    <sl:tag rdf:resource="&tag;julie_grollier"/>
  </rdf:Description>
  <rdf:Description rdf:about="la_plus_grosse_explosion_jamais">
    <sl:bookmarkOf rdf:resource="https://www.sciencesetavenir.fr/espace/univers/un-trou-noir-responsable-de-la-plus-grosse-explosion-jamais-observee-dans-l-espace-depuis-le-big-bang_142029"/>
    <sl:creationTime>2020-03-01T12:15:45Z</sl:creationTime>
    <sl:creationDate>2020-03-01</sl:creationDate>
    <dc:title>La plus grosse explosion jamais observée depuis le Big Bang</dc:title>
    <sl:comment>L'événement fut si puissant qu'il aurait créé une brèche de la taille de 15 Voies lactées réunies dans le plasma environnant. &#xD;
&#xD;
&gt; L'Univers est un endroit étrange.</sl:comment>
    <sl:tag rdf:resource="&tag;astrophysique"/>
  </rdf:Description>
  <rdf:Description rdf:about="_2003_08271_pre_trained_models">
    <sl:tag rdf:resource="&tag;arxiv_doc"/>
    <sl:arxiv_num>2003.08271</sl:arxiv_num>
    <sl:arxiv_author>Ning Dai</sl:arxiv_author>
    <sl:arxiv_author>Yunfan Shao</sl:arxiv_author>
    <sl:arxiv_title xml:lang="en">Pre-trained Models for Natural Language Processing: A Survey</sl:arxiv_title>
    <sl:arxiv_firstAuthor>Xipeng Qiu</sl:arxiv_firstAuthor>
    <sl:arxiv_author>Yige Xu</sl:arxiv_author>
    <sl:arxiv_updated>2020-04-24T15:16:59Z</sl:arxiv_updated>
    <sl:tag rdf:resource="&tag;survey"/>
    <sl:arxiv_published>2020-03-18T15:22:51Z</sl:arxiv_published>
    <sl:arxiv_author>Xuanjing Huang</sl:arxiv_author>
    <sl:bookmarkOf rdf:resource="https://arxiv.org/abs/2003.08271"/>
    <dc:title xml:lang="en">[2003.08271] Pre-trained Models for Natural Language Processing: A Survey</dc:title>
    <sl:tag rdf:resource="&tag;pre_trained_language_models"/>
    <sl:arxiv_author>Xipeng Qiu</sl:arxiv_author>
    <sl:arxiv_author>Tianxiang Sun</sl:arxiv_author>
    <sl:arxiv_summary xml:lang="en">Recently, the emergence of pre-trained models (PTMs) has brought natural
language processing (NLP) to a new era. In this survey, we provide a
comprehensive review of PTMs for NLP. We first briefly introduce language
representation learning and its research progress. Then we systematically
categorize existing PTMs based on a taxonomy with four perspectives. Next, we
describe how to adapt the knowledge of PTMs to the downstream tasks. Finally,
we outline some potential directions of PTMs for future research. This survey
is purposed to be a hands-on guide for understanding, using, and developing
PTMs for various NLP tasks.</sl:arxiv_summary>
    <sl:creationTime>2020-03-19T13:34:50Z</sl:creationTime>
    <sl:creationDate>2020-03-19</sl:creationDate>
  </rdf:Description>
</rdf:RDF>
